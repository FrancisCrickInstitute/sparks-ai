<!doctype html>

<html lang="en">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <title>SPARKS Documentation</title>
    <meta name="description" content="A simple HTML/CSS DocumentationTemplate">
    <meta name="author" content="Nicolas Skatchkovsky">

    <link rel="stylesheet" href="style.css?">
    <link href="https://fonts.googleapis.com/css2?family=Noto+Serif:wght@400;700&family=Open+Sans:ital,wght@0,400;0,700;1,600&display=swap" rel="stylesheet">
    
    <script defer src="https://use.fontawesome.com/releases/v5.7.2/js/all.js" integrity="sha384-0pzryjIRos8mFBWMzSSZApWtPl/5++eIfzYmTgBBmXYdhvxPc+XcFEk+zJwDgWbP" crossorigin="anonymous"></script>

    <style>
      #code-block {
        background-color: #333; /* This is a dark gray color. You can use any color you prefer. */
        color: #fff; /* This is a white color for the text. You can use any color you prefer. */
        padding: 2px; /* This adds some padding around the text. You can adjust this value as needed. */
        border-radius: 2px; /* This creates rounded corners with a radius of 10 pixels. You can adjust this value as needed. */        
      }
    </style>

    <style>
      .big-code-block {
        background-color: #333;
        color: #fff;
        padding: 3px;
        border-radius: 1px;
        position: relative;
      }
      .copy-button {
        position: absolute;
        top: 10px;
        right: 10px;
        background-color: #fff;
        color: #333;
        border: none;
        padding: 5px 10px;
        border-radius: 5px;
        cursor: pointer;
      }
    </style>

</head>

<body>
    <div class="navbar clear nav-top">
        <div class="row content">
            <a class="right" style="text-decoration: underline;" href="https://github.com/franciscrickinstitute/sparks"><i class="fab fa-github"></i>&nbsp; GitHub</a>
            <a class="right" style="text-decoration: underline;" href="https://franciscrickinstitute.github.io/sparks-ai/"><i class="fas fa-book"></i>&nbsp; Main page</a>
            <a class="right" style="text-decoration: underline;" href="https://franciscrickinstitute.github.io/sparks-ai/demos#"><i class="fas fa-laptop-code"></i>&nbsp; Demos</a>
            <a class="right" href="mailto:nicolas.skatchkovsky[at]crick[dot]ac[dot]uk" target="_blank"><i class="fas fa-paper-plane"></i>&nbsp; nicolas.skatchkovsky[at]crick[dot]ac[dot]uk</a>
        </div>
    </div>

    <div class="container clear">
        <div class="row wrapper">

            <div class="sidepanel">
                 <img width="60%" src="static/images/logo_2.png" alt="Image description">

                <a class="title" href="#">Introduction</a>
                
                <a class="section" href="#">About this library</a>

                <div class="divider left"></div>

                <a class="title" href="#gettingstarted">Getting Started</a>

                <a class="section" href="#pipinstall">Installation with pip</a>
                <a class="section" href="#condainstall">Installation with conda</a>

                <div class="divider left"></div>

                <a class="title" href="#minimumexample">Minimum example</a>

                <div class="divider left"></div>

                <a class="title" href="#data">Data</a>

                <a class="section" href="#data">Support</a>
                <a class="section" href="#dataprocessing">Processing</a>
                <div class="divider left"></div>

                <a class="title" href="#training">Training</a>
                <a class="section" href="#online">Online training</a>
                <a class="section" href="#multisession">Multi-session training</a>

                <div class="divider left"></div>

                <a class="title" href="#comingsoon">Coming soon!</a>
            
             <div class="space double"></div>

            </div>

            <div class="right-col">

                  <div class="container" style="width: 60%; margin-bottom: 20px;">
                     <img width="60%" src="static/images/logo_2.png" alt="Image description">
                 </div>

                <h1 >Introduction</h1>
                
                <p> This is the official documentation for SPARKS, a Sequential Predictive Autoencoder for the Representation of Spiking Signals. SPARKS combines a variational autoencoder with a novel Hebbian attention layer and predictive learning rule to obtain consistent latent embeddings that capture most of the variance from high dimensional neuronal responses. </p> 
            
                <h2>About this library</h2>
                <p id="technology"> <code id="code-block">sparks</code> is built in python (v >= 3.9). using Pytorch. We provide high-level training/testing function to get quickly started, and users that are already familiar with Pytorch should be able to easily make full use of SPARKS. The following instructions were tested on a Linux server (Rocky Linux 8.7 Green Obsidian) with CUDA 12.2 and MacOS (Sonoma 14.5 with Apple M2 Max).</p>

                
        <div class="divider" style="width:24%; margin:30px 0;"></div>

                <h1 id="gettingstarted">Getting Started</h1>

                <h2>Installation</h2>
                
                <p>SPARKS can be installed via <code id="code-block">pip</code> or <code id="code-block">conda</code>.</p>
                
                <h3 id="pipinstall"> Installation with pip </h3>
                   <ol>
                    <li>
                     Create a new virtual environment by running
                     <div class="big-code-block">
                      <pre><code id="code">$ virtualenv .env && source .env/bin/activate</code></pre>
                      <button class="copy-button">Copy</button>
                    </div>
                    <script src="script.js"></script>
                    </li>
                    <br>
                    <li>
                     Depending on your systems requirements, you may want to install Pytorch manually, for instance to install the correct CUDA version (please see more details <a href="https://pytorch.org/get-started/locally/">here</a>) .
                    </li>
                    <br id="openingapp">
                    <li>
                     Install using conda: 
                     <div class="big-code-block">
                      <pre><code id="code">$ pip install sparks</code></pre>
                      <button class="copy-button">Copy</button>
                    </div>
                    <script src="script.js"></script>
                    </li>
                    <br>
                </ol>
                

                <h3 id= "condainstall"> Installation with conda (recommended)</h3>
                   <ol>
                    <li>
                     Install Anaconda or Miniconda (please see more details <a href="https://docs.anaconda.com/anaconda/install/">here</a>).
                    </li>
                    <br>
                    <li>
                    Create a new conda environment by running
                    <div class="big-code-block">
                      <pre><code id="code">$ conda create -n sparks python==3.9
$ conda activate sparks</code></pre>
                      <button class="copy-button">Copy</button>
                    </div>
                    <script src="script.js"></script>
                    </li>
                    <li>
                     Depending on your systems requirements, you may want to install Pytorch manually, for instance to install the correct CUDA version (please see more details <a href="https://pytorch.org/get-started/locally/">here</a>) .
                    </li>
                    <br id="openingapp">
                    <li>
                     Install using conda: 
                     <div class="big-code-block">
                      <pre><code id="code">$ conda install sparks</code></pre>
                      <button class="copy-button">Copy</button>
                    </div>
                    <script src="script.js"></script>
                    </li>
                    <br>
                </ol>

                <p id="installingapp"><b>Important:</b> By default, <code id="code-block">sparks</code> is installed without the dependencies required to run scripts using the monkey reaching and Allen datasets (Chowdury et al., 2020, de Vries et al., 2020). To install the library with all dependencies, please run <code id="code-block"> pip install sparks[scripts]</code>. However, due to compatibility issues with the <code id="code-block">allen_sdk</code> library, we recommend installing it first from this <a href="https://allensdk.readthedocs.io">link</a>.</p>

            <div class="divider" style="width:24%; margin:30px 0;"></div>
            
            
                <h1 id="minimumexample">Minimum example</h1>
                
                <p> <code id="code-block">sparks</code> follows standard <a href="https://pytorch.org/">PyTorch</a> syntax, and scripts will often look like this:</p>
                <div class="big-code-block">
                    <pre><code id="code">
train_dl = MyTrainDataLoader()  # the train/test functions expect a list of dls, one for each session
test_dl = MyTrainDataLoader()
encoding_network = HebbianTransformerEncoder(n_neurons_per_sess=n_neurons,  # number of neurons in the recording(s)
                                             embed_dim=embed_dim,  #  attention embedding dimension
                                             latent_dim=latent_dim,  # latent dimension
                                             tau_s_per_sess=tau_s,  # STDP decay(s) in seconds, one per attention head
                                             dt_per_sess=dt,  # sampling period
                                             n_layers=n_layers,  # number of conventional attention layers
                                             n_heads=n_heads,  # number of attention heads,  
                                             ).to(device)
n_inputs_decoder = latent_dim * tau_p
decoding_network = mlp(in_dim=n_inputs_decoder,
                       output_dim_per_session=input_size).to(device)

optimizer = torch.optim.Adam(list(encoding_network.parameters())
                             + list(decoding_network.parameters()), lr=lr)
loss_fn = torch.nn.BCEWithLogitsLoss()

for epoch in tqdm.tqdm(range(args.n_epochs)):
    train(encoder=encoding_network,
                   decoder=decoding_network,
                   train_dls=[train_dl],
                   loss_fn=loss_fn,
                   optimizer=optimizer,
                   latent_dim=latent_dim,
                   tau_p=tau_p,  # past time-steps window
                   tau_f=tau_f,  # future time-steps window
                   beta=beta,  # KLD regularisation strength
                   device=device)

    if (epoch + 1) % test_period == 0:
        test_loss, encoder_outputs, decoder_outputs = test(encoding_network,
                                                           decoding_network,
                                                           test_dl,
                                                           latent_dim=latent_dim,
                                                           tau_p=tau_p,
                                                           tau_f=tau_f,
                                                           loss_fn=loss_fn,
                                                           device=device,)</code></pre>
                    <button class="copy-button">Copy</button>
                    </div>
                    <script src="script.js"></script>


                <div class="divider" style="width:24%; margin:30px 0;"></div>
                
                <h1 id= "data">Data</h1>

                <h2>Support</h2>

                <p><code id="code-block">sparks</code> supports both electrophysiology and calcium imaging data. Make sure to select the correct data type with the <code id="code-block">data_type</code> argument during the encoder initialisation:</p>
                <div class="big-code-block">
                    <pre><code id="code">
my_ephys_encoder = HebbianTransformerEncoder(n_neurons_per_sess=n_neurons,  # number of neurons in the recording(s)
                                             embed_dim=embed_dim,  #  attention embedding dimension
                                             latent_dim=latent_dim,  # latent dimension
                                             tau_s_per_sess=tau_s,  # STDP decay(s) in seconds, one per attention head
                                             dt_per_sess=dt,  # sampling period
                                             n_layers=n_layers,  # number of conventional attention layers
                                             n_heads=n_heads,  # number of attention heads,
                                             data_type='ephys'  # by default  
                                             ).to(device)


my_ca_encoder = HebbianTransformerEncoder(n_neurons_per_sess=n_neurons,  # number of neurons in the recording(s)
                                          embed_dim=embed_dim,  #  attention embedding dimension
                                          latent_dim=latent_dim,  # latent dimension
                                          tau_s_per_sess=tau_s,  # STDP decay(s) in seconds, one per attention head
                                          dt_per_sess=dt,  # sampling period
                                          n_layers=n_layers,  # number of conventional attention layers
                                          n_heads=n_heads,  # number of attention heads,
                                          data_type='ca'  
                                          ).to(device)
</code></pre>
                    <button class="copy-button">Copy</button>
                    </div>
                    <script src="script.js"></script>


                <h2 id= "dataprocessing">Processing</h2>
                
                <p>The <code id="code-block">train</code> and <code id="code-block">test</code> functions rely on the user to provide a list of PyTorch <code id="code-block">Dataloaders</code>, one for each session to train/test on.</p>

                <p>In the underlying Pytorch <code id="code-block">Dataset</code> (or any iterable), examples should be formatted with shape <code id="code-block">NxT</code> for the neural signals and <code id="code-block">CxHxWx...xT</code> for targets during supervised learning, with <code id="code-block">N</code> the number of neurons, <code id="code-block">T</code> the number of time-steps, and <code id="code-block">CxHxWx...</code> the arbitrary dimensions of the target signal.</p>

                <div class="divider" style="width:24%; margin:30px 0;"></div>
                
                <h1 id= "training">Training</h1>

                <h2 id= "training">Online training</h2>

                <p>Training relies on backpropagation-through-time (BPTT), for which memory requirements scale quadratically with the number of time-steps.</p>

                <p>To allow training on long examples, we support an online version of the algorithm that ignores the dependency of the current encoder state on previous time-steps, and has fixed complexity w.r.t. the number of time-steps. This can be used by using the <code id="code-block">online</code> argument of the  <code id="code-block">train</code> function:</p>
                <div class="big-code-block">
                    <pre><code id="code">
train(encoder=encoding_network,
      decoder=decoding_network,
      train_dls=[train_dl],
      loss_fn=loss_fn,
      optimizer=optimizer,
      latent_dim=latent_dim,
      tau_p=tau_p,  # past time-steps window
      tau_f=tau_f,  # future time-steps window
      beta=beta,  # KLD regularisation strength
      online=True,  # False by default
      device=device) </code></pre>
                    <button class="copy-button">Copy</button>
                    </div>
                    <script src="script.js"></script>

                <h2 id= "multisession">Multi session training</h2>
                <p>When training on multiple sessions, SPARKS trains one Hebbian attention layer per session. To resolve the correct layer at runtime, each is assigned a unique id, which are by default zero-indexed:</p>
                    <div class="big-code-block">
                    <pre><code id="code">
train_dls = [MyTrainDl(session) for session in [session_1, session_2, session_3]]
sessions_ids = [111, 222, 333]
n_neurons_per_sess = [10, 20, 30]

encoding_network = HebbianTransformerEncoder(n_neurons_per_sess=n_neurons,  # number of neurons in the recording(s)
                                             embed_dim=embed_dim,  #  attention embedding dimension
                                             latent_dim=latent_dim,  # latent dimension
                                             tau_s_per_sess=tau_s,  # STDP decay(s) in seconds, one per attention head
                                             dt_per_sess=dt,  # sampling period
                                             n_layers=n_layers,  # number of conventional attention layers
                                             n_heads=n_heads,  # number of attention heads,
                                             ).to(device)

print(encoding_network.id_per_sess) # outputs [0, 1, 2]

# the training function will automatically assign ids [0, 1, 2] to each session and resolve them
# against those saved by the model at runtime
test(encoder=encoding_network,
     decoder=decoding_network,
     test_dls=train_dls,
     loss_fn=loss_fn,
     optimizer=optimizer,
     latent_dim=latent_dim,
     tau_p=tau_p,  # past time-steps window
     tau_f=tau_f,  # future time-steps window
     beta=beta,  # KLD regularisation strength
     online=True,  # False by default
     device=device) </code></pre>
                <button class="copy-button">Copy</button>
                    </div>
                    <script src="script.js"></script>

    <p>You can also define custom ids for your sessions, and pass them to the test or train function:</p>
                <div class="big-code-block">
                    <pre><code id="code">

test_dls = [MyTestDl(session) for session in [session_1, session_2, session_3]]
sessions_ids = [111, 222, 333]


encoding_network = HebbianTransformerEncoder(n_neurons_per_sess=n_neurons,  # number of neurons in the recording(s)
                                             embed_dim=embed_dim,  #  attention embedding dimension
                                             latent_dim=latent_dim,  # latent dimension
                                             tau_s_per_sess=tau_s,  # STDP decay(s) in seconds, one per attention head
                                             dt_per_sess=dt,  # sampling period
                                             n_layers=n_layers,  # number of conventional attention layers
                                             n_heads=n_heads,  # number of attention heads
                                             id_per_sess=sessions_ids  
                                             ).to(device)

train(encoder=encoding_network,
               decoder=decoding_network,
               train_dls=[train_dl],
               loss_fn=loss_fn,
               optimizer=optimizer,
               latent_dim=latent_dim,
               tau_p=tau_p,  # past time-steps window
               tau_f=tau_f,  # future time-steps window
               beta=beta,  # KLD regularisation strength
               online=True,  # False by default
               sess_ids=sessions_ids,
               device=device) </code></pre>
                    <button class="copy-button">Copy</button>
                    </div>
                    <script src="script.js"></script>

                <div class="divider" style="width:24%; margin:30px 0;"></div>

                <h1 id= "comingsoon">Coming soon!</h1>

                <p>Sparse attention for scaling up the number of neurons.</p>

                <div class="divider" style="width:24%; margin:30px 0;"></div>
                

            </div>

        </div>


    </div>


</body>

</html>